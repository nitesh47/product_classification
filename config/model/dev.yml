---
version: 0.1

random_state: 42

data:
  val_size: 0.2

encoding:
  max_length: 32
  batch_size: 32

transformers:
  tokenizer: distilbert-base-uncased
  model: distilbert-base-uncased
  activation: softmax

train:
  batch_size: 32
  epochs: 50
  optimization:
    init_lr: 0.00001
  callbacks:
    verbose: true
    early_stopping:
      min_delta: 0
      patience: 1
    reduce_on_plateau:
      factor: 0.2
      patience: 1
      min_lr: 0.000001


inference:
  batch_size: 32
