{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5454ba3-fbfc-4b4d-8c81-6aa3cf651c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8b3f95-620f-4113-937f-84d7d50f2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This functions combine multiple CSV files into a single CSV file\n",
    "    :param data_path: GCS path where all the cleaned preprocessed csv file saved\n",
    "    :return: Dataframe\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(\".log\"):\n",
    "            filepath = os.path.join(data_path, filename)\n",
    "            data = pd.read_json(filepath, lines=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9ee51-a70f-430c-a056-12c7c5e9596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_json(\"predict_category/events.log\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ba182-7b05-442d-93ba-21b5f49abacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f42f7-7327-480b-a01d-c556d62bc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c262d-4da3-4348-865d-b62ea9c8d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.category.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751468f-43fe-4d5d-b7c1-16690dd8c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.loc[(file.name == \"search\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976eb75-8fa2-41b3-ac95-dee4d6d77a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.loc[(file[\"name\"] == \"search\") & (pd.isna(file[\"title\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e35d5-04e5-4afe-849d-eae0f3679c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.loc[(file[\"name\"] == \"search\") & (pd.notna(file[\"title\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44ed5a-c8f0-474a-9d65-78b3afbb32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = file.loc[(pd.isna(file[\"category\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3a5d2-f189-438a-bf3a-1375b939c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c874471f-d007-45a4-bdc0-16d1cd56bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = file.loc[(pd.notna(file[\"category\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d08a3-dcb6-4bae-b291-ccbeab7102b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.name == \"search\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af780c-ec98-4f1c-8c84-f151be379579",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc4adf-1dfb-41b8-b293-e8f9609d60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"category\"].value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844eb057-be80-40b6-be87-120be4217e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"category\"].value_counts().plot.pie();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b674a-a834-40f8-9646-f8f86eb52c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[\"category\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79978549-d81f-41b3-815a-b6eda426bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = len(data[\"title\"].apply(lambda x: str(x).casefold().strip()).unique()) / len(\n",
    "    data\n",
    ")\n",
    "print(f\"The percentage of the unique title is {round(perc*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc2c65-7627-4844-8430-70f3f9eb1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords = data[\"title\"].apply(lambda x: len(str(x).split()))\n",
    "nwords.plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9d364-8848-428f-be82-fa0664650842",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b2446-012a-4382-896d-7d8c4bea0188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cb9d7-99f0-4081-bd9b-6feb05e89e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c6379-851f-4050-a4e0-52fd1ac799f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368eb128-337b-4ee6-9b6c-73df9d168db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_and_training_data(df: pd.DataFrame):\n",
    "    test_data = df.loc[(pd.isna(df[\"category\"]))]\n",
    "    data = df.loc[(pd.notna(df[\"category\"]))]\n",
    "    return test_data, data\n",
    "\n",
    "\n",
    "def clean_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function cleans the labels for the training.\n",
    "    :param df: training dataframe\n",
    "    :return: a dataframe with lowered label column.\n",
    "    \"\"\"\n",
    "    df[LABEL_COLUMN_NAME] = df[LABEL_COLUMN_NAME].replace(\"\", np.nan)\n",
    "    df = df.dropna(subset=[LABEL_COLUMN_NAME])\n",
    "    df = df.reset_index(drop=True)\n",
    "    logging.info(\n",
    "        f\"Missing labels dropped successfully: \\\n",
    "                    Data has a total of {len(df)} rows.\"\n",
    "    )\n",
    "    df[LABEL_COLUMN_NAME] = df[LABEL_COLUMN_NAME].str.lower()\n",
    "    df[LABEL_COLUMN_NAME] = df[LABEL_COLUMN_NAME].str.strip()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def text_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    This function cleans text for English language,\n",
    "    as it applies several rules,\n",
    "    e.g. remove all special characters, remove duplicates etc.\n",
    "    :param text: text to be preprocessed\n",
    "    :return: the preprocessed text\n",
    "    \"\"\"\n",
    "    text = text.strip().casefold()\n",
    "    text = re.sub(r\"[-()?@.,;_#+*'‘{}%$§!<>/]\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"\\r\\n|\\r|\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.rstrip()\n",
    "    text = re.sub(r\"[\\n\\t\\ ]+\", \" \", text).split()\n",
    "    text = list(dict.fromkeys(text))\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def apply_text_preprocessing(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This functions combines product_name and menu_category\n",
    "    columns into a new column and perform\n",
    "    text preprocessing on the new column.\n",
    "    :param df: training dataframe\n",
    "    :return: the dataframe with the cleaned\n",
    "     preprocessed text int the new column\n",
    "    \"\"\"\n",
    "    logging.info(\"Start combining product_name and menu_category columns... \")\n",
    "    df[TRAINING_PRODUCT_NAME] = df[TRAINING_PRODUCT_NAME].replace(np.nan, \"\")\n",
    "    df = df.dropna(subset=[TRAINING_PRODUCT_NAME, TRAINING_MENU_CATEGORY])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[FEATURE_COLUMN_NAME] = df[COMB_P_M].apply(text_preprocessing)\n",
    "    logging.info(f\"Data preprocessing finished successfully.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_preprocessing_training(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    This function runs all required steps for\n",
    "    the preprocessing of the product name and menu category\n",
    "    :param df: training dataframe\n",
    "    :return: a dataframe with preprocessed data.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_df = apply_text_preprocessing(df)\n",
    "    logging.info(\"Start dropping empty cleaned combined product names values... \")\n",
    "    cleaned_df[FEATURE_COLUMN_NAME] = cleaned_df[FEATURE_COLUMN_NAME].replace(\n",
    "        \"\", np.nan\n",
    "    )\n",
    "    cleaned_df = cleaned_df.dropna(subset=[FEATURE_COLUMN_NAME])\n",
    "    cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "    cleaned_df = cleaned_df.drop_duplicates(subset=FEATURE_COLUMN_NAME, keep=\"first\")\n",
    "    return cleaned_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
