{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93beae4e-817f-4483-a5fe-0039d01f7c13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Using cached huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.25.0 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4080ddc5-907c-4fa3-a623-14ddf27a7837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 10:55:36.995372: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 10:55:38.277027: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-09-23 10:55:38.277162: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-09-23 10:55:38.277175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Any, Dict, Iterable, List, Tuple\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    cohen_kappa_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import yaml\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea4ca6a-0349-4a11-aa09-fb63a8c3b367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURE_COLUMN_NAME = \"clean_text\"\n",
    "LABEL_COLUMN_NAME = \"category\"\n",
    "TITLE = \"title\"\n",
    "QUERY = \"query\"\n",
    "MAX_LENGTH = \"max_length\"\n",
    "DATA = \"data\"\n",
    "VAL_SIZE = \"val_size\"\n",
    "RANDOM_STATE = \"random_state\"\n",
    "TRANSFORMERS = \"transformers\"\n",
    "TOKENIZER = \"tokenizer\"\n",
    "ENCODING = \"encoding\"\n",
    "TRAIN = \"train\"\n",
    "BATCH_SIZE = \"batch_size\"\n",
    "MODEL = \"model\"\n",
    "OPTIMIZATION = \"optimization\"\n",
    "INIT_LR = \"init_lr\"\n",
    "CALLBACKS = \"callbacks\"\n",
    "VERBOSE = \"verbose\"\n",
    "VAL_LOSS = \"val_loss\"\n",
    "REDUCE_ON_PLATEAU = \"reduce_on_plateau\"\n",
    "PATIENCE = \"patience\"\n",
    "MIN_LR = \"min_lr\"\n",
    "MIN_DELTA = \"min_delta\"\n",
    "METRICS_FULL_REPORT = \"full_report\"\n",
    "MATHEW = \"Mathew\"\n",
    "FACTOR = \"factor\"\n",
    "EPOCHS = \"epochs\"\n",
    "EARLY_STOPPING = \"early_stopping\"\n",
    "COHEN = \"Cohen\"\n",
    "ACTIVATION = \"activation\"\n",
    "ACCURACY = \"Accuracy\"\n",
    "INPUT_IDS = \"input_ids\"\n",
    "ENCODED_LABEL = \"encoded_label\"\n",
    "LABEL_ENCODER_PKL = \"label_encoder.pkl\"\n",
    "METRICS_JSON = \"metrics.json\"\n",
    "PREDICTED_CATEGORY = \"predicted_catagory\"\n",
    "METRICS_DIR = \"metrics\"\n",
    "INFERENCE = \"inference\"\n",
    "PROB_SCORES = \"prediction_probability\"\n",
    "FILE_NAME_PREFIX = \"search_data_prediction_file\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9843a81-bf5f-4299-a7e9-7b1b540cab9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_csv_file(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This functions combine multiple CSV files into a single CSV file\n",
    "    :param data_path: GCS path where all the cleaned preprocessed csv file saved\n",
    "    :return: Dataframe\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(data_path, filename)\n",
    "            data = pd.read_csv(filepath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd3c48-2fbb-4e56-921c-fd353c902a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model_params: Dict[str, Any]):\n",
    "        self.model_params = model_params\n",
    "        self.tokenizer = self.get_tokenizer()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def get_tokenizer(self) -> Any:\n",
    "        try:\n",
    "            logger.info(\"Download the tokenizer from Huggingface.\")\n",
    "            return DistilBertTokenizer.from_pretrained(self.model_params[TRANSFORMERS][TOKENIZER])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            logger.info(\"Perform label encoding.\")\n",
    "            df[ENCODED_LABEL] = self.label_encoder.fit_transform(list(df[LABEL_COLUMN_NAME].values))\n",
    "            logger.info(\n",
    "                f\"The total number of samples: {len(df[ENCODED_LABEL])}; classes: {len(self.label_encoder.classes_)}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in label encoding: {e}\")\n",
    "            raise\n",
    "    def split_to_train_val(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        try:\n",
    "            logger.info(\"Perform train/validation split.\")\n",
    "            return train_test_split(\n",
    "                df,\n",
    "                test_size=self.model_params[DATA][VAL_SIZE],\n",
    "                random_state=self.model_params[RANDOM_STATE],\n",
    "                stratify=list(df[LABEL_COLUMN_NAME]),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during train/validation split: {e}\")\n",
    "            raise\n",
    "\n",
    "    def tokenize_batch(self, texts: List[str], batch_size: int = 32) -> Any:\n",
    "        try:\n",
    "            logger.info(\"Tokenizing batch of texts.\")\n",
    "            tokenized_data = []\n",
    "            for i in range(0, len(texts), self.model_params[ENCODING][BATCH_SIZE]):\n",
    "                batch_text = texts[i:i + self.model_params[ENCODING][BATCH_SIZE]]\n",
    "                tokenized_batch = self.tokenizer(batch_text, max_length=self.model_params[ENCODING][MAX_LENGTH],\n",
    "                                                 padding='max_length', truncation=True, return_tensors='tf')['input_ids']\n",
    "                tokenized_data.append(tokenized_batch)\n",
    "            return tf.concat(tokenized_data, axis=0)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during tokenization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def build_and_train_model(self, train_dataset, val_dataset, callbacks: List[Any], n_steps):\n",
    "        \"\"\"\n",
    "        Build and train the model using TFDistilBertModel and TensorFlow's Keras API.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Building the model.\")\n",
    "            bert_model = TFDistilBertModel.from_pretrained(self.model_params[TRANSFORMERS][MODEL])\n",
    "\n",
    "            input_ids = tf.keras.layers.Input(shape=(self.model_params[ENCODING][MAX_LENGTH],), dtype='int32')\n",
    "            bert_output = bert_model(input_ids)[0]\n",
    "            cls_token = bert_output[:, 0, :]\n",
    "            output = tf.keras.layers.Dense(len(self.label_encoder.classes_), activation='softmax')(cls_token)\n",
    "\n",
    "            model = tf.keras.models.Model(inputs=input_ids, outputs=output)\n",
    "\n",
    "            logger.info(\"Compiling the model.\")\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.model_params[TRAIN][OPTIMIZATION][INIT_LR]),\n",
    "                          loss=SparseCategoricalCrossentropy(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            logger.info(\"Training the model.\")\n",
    "            train_start = time.time()\n",
    "            model.fit(train_dataset,\n",
    "                      validation_data=val_dataset,\n",
    "                      steps_per_epoch=n_steps,\n",
    "                      epochs=self.model_params[TRAIN][EPOCHS],\n",
    "                      batch_size=self.model_params[TRAIN][BATCH_SIZE],\n",
    "                      callbacks=callbacks)\n",
    "            train_time_minutes = round((time.time() - train_start) / 60, 2)\n",
    "            logger.info(f\"The training has finished, took {train_time_minutes} minutes.\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during model building or training: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model: Any, label_encoder: LabelEncoder):\n",
    "        self.model = model\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def calculate_scores(self, X_val: Iterable[int], y_val_true: Iterable[int]) -> Dict[str, float]:\n",
    "        try:\n",
    "            logger.info(\"Calculating the metrics.\")\n",
    "            prediction = self.model.predict(X_val)\n",
    "            y_val_pred = tf.argmax(prediction, axis=1).numpy()\n",
    "            y_val_pred = self.label_encoder.inverse_transform(y_val_pred)\n",
    "            y_val_true = self.label_encoder.inverse_transform(y_val_true)\n",
    "            metrics = self.get_metrics(y_val_true, y_val_pred)\n",
    "            logger.info(f\"Metrics: {metrics}\")\n",
    "            return y_val_pred, y_val_true, metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during score calculation: {e}\")\n",
    "            raise\n",
    "\n",
    "    def val_data(self, df_val, y_pred):\n",
    "        df_val[PREDICTED_CATEGORY] = y_pred\n",
    "        return df_val\n",
    "\n",
    "    def get_metrics(self,\n",
    "                    true_values: Iterable[str],\n",
    "                    predicted_values: Iterable[str],\n",
    "                    round_n: int = 3,\n",
    "                    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Prints classification report with accuracy.\n",
    "        :param true_values: Iterable with the actual values.\n",
    "        :param predicted_values: Iterable with the predicted values.\n",
    "        :param round_n: The number after the decimal point.\n",
    "        :return: The dictionary with the model metrics.\n",
    "        \"\"\"\n",
    "        metrics = dict()\n",
    "\n",
    "        acc_value = accuracy_score(true_values, predicted_values)\n",
    "        metrics[ACCURACY] = round(acc_value, round_n)\n",
    "        cohen = cohen_kappa_score(true_values, predicted_values)\n",
    "        metrics[COHEN] = round(cohen, round_n)\n",
    "        matthew = matthews_corrcoef(true_values, predicted_values)\n",
    "        metrics[MATHEW] = round(matthew, round_n)\n",
    "\n",
    "        f1_macro = f1_score(true_values, predicted_values, average=\"macro\")\n",
    "        metrics[\"f1-score macro\"] = round(f1_macro, round_n)\n",
    "        precision_macro = precision_score(true_values, predicted_values, average=\"macro\")\n",
    "        metrics[\"Precision macro\"] = round(precision_macro, round_n)\n",
    "        recall_macro = recall_score(true_values, predicted_values, average=\"macro\")\n",
    "        metrics[\"Recall macro\"] = round(recall_macro, round_n)\n",
    "\n",
    "        f1_micro = f1_score(true_values, predicted_values, average=\"micro\")\n",
    "        metrics[\"f1-score micro\"] = round(f1_micro, round_n)\n",
    "        precision_micro = precision_score(true_values, predicted_values, average=\"micro\")\n",
    "        metrics[\"Precision micro\"] = round(precision_micro, round_n)\n",
    "        recall_micro = recall_score(true_values, predicted_values, average=\"micro\")\n",
    "        metrics[\"Recall micro\"] = round(recall_micro, round_n)\n",
    "\n",
    "        f1_weighted = f1_score(true_values, predicted_values, average=\"weighted\")\n",
    "        metrics[\"f1-score weighted\"] = round(f1_weighted, round_n)\n",
    "        precision_weighted = precision_score(\n",
    "            true_values, predicted_values, average=\"weighted\"\n",
    "        )\n",
    "        metrics[\"Precision weighted\"] = round(precision_weighted, round_n)\n",
    "        recall_weighted = recall_score(true_values, predicted_values, average=\"weighted\")\n",
    "        metrics[\"Recall weighted\"] = round(recall_weighted, round_n)\n",
    "\n",
    "        metrics[METRICS_FULL_REPORT] = classification_report(\n",
    "            true_values, predicted_values, output_dict=True\n",
    "        )\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class ArtifactManager:\n",
    "    def __init__(self, artifacts_dir: str):\n",
    "        self.artifacts_dir = artifacts_dir\n",
    "\n",
    "    def save_artifacts(self, model: Any, label_encoder: LabelEncoder, metrics: Dict[str, float], df_val_pred) -> None:\n",
    "        try:\n",
    "            joblib.dump(label_encoder, os.path.join(self.artifacts_dir, LABEL_ENCODER_PKL))\n",
    "            metrics_dir_path = os.path.join(self.artifacts_dir, METRICS_DIR)\n",
    "            os.makedirs(metrics_dir_path, exist_ok=True)\n",
    "            with open(os.path.join(metrics_dir_path, METRICS_JSON), 'w') as f:\n",
    "                json.dump(metrics, f, ensure_ascii=False)\n",
    "            model.save(self.artifacts_dir)\n",
    "            df_val_pred.to_csv(os.path.join(metrics_dir_path, \"val_predicted.csv\"), index=False)\n",
    "            logger.info(\"The artifacts are successfully saved.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving artifacts: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def get_callbacks(callback_params: Dict[str, Any]) -> List[Any]:\n",
    "\n",
    "    \"\"\"\n",
    "    A custom function to provide the needed callbacks based on the parameters specified.\n",
    "    :param callback_params: The dictionary with the callback params.\n",
    "    :return: The list of Tensorflow callbacks.\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "    if EARLY_STOPPING in callback_params:\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=VAL_LOSS,\n",
    "            verbose=callback_params[VERBOSE],\n",
    "            min_delta=callback_params[EARLY_STOPPING][MIN_DELTA],\n",
    "            patience=callback_params[EARLY_STOPPING][PATIENCE],\n",
    "            mode=\"auto\",\n",
    "        )\n",
    "        callbacks.append(early_stopping)\n",
    "    if REDUCE_ON_PLATEAU in callback_params:\n",
    "        reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=VAL_LOSS,\n",
    "            verbose=callback_params[VERBOSE],\n",
    "            factor=callback_params[REDUCE_ON_PLATEAU][FACTOR],\n",
    "            patience=callback_params[REDUCE_ON_PLATEAU][PATIENCE],\n",
    "            min_lr=callback_params[REDUCE_ON_PLATEAU][MIN_LR],\n",
    "        )\n",
    "        callbacks.append(reduce_on_plateau)\n",
    "    logger.info(f\"Callbacks: {callbacks}\")\n",
    "    return callbacks\n",
    "def transform_data_for_training(x_train, y_train, x_val, y_val, model_params):\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(2048)\n",
    "        .batch(model_params[TRAIN][BATCH_SIZE])\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    val_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "        .batch(model_params[TRAIN][BATCH_SIZE])\n",
    "        .cache()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def load_model_params(model_params_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    The purpose of this function is to load the model parameters from the provided path.\n",
    "    :param model_params_path: GCS path for the model parameters\n",
    "    :return: A dictionary containing the model parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f'Read model params file from: \"{model_params_path}\".')\n",
    "        model_params = None\n",
    "        with file_io.FileIO(model_params_path, \"r\") as f:\n",
    "            model_params = yaml.safe_load(f)\n",
    "        if not model_params:\n",
    "            raise Exception(\n",
    "                f\"Failed to load the model parameters file in the\"\n",
    "                f'following path: \"{model_params_path}\", the file was not found.'\n",
    "            )\n",
    "        logger.info(f\"The model params:\\n{model_params}\")\n",
    "        logger.info(\"Model parameters loaded successfully.\")\n",
    "        return model_params\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model parameters: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def train_main(data_path: str, model_params_path: str, artifacts_dir: str):\n",
    "    try:\n",
    "        model_params = load_model_params(model_params_path)\n",
    "        df = read_csv_file(data_path)\n",
    "        df = df.drop_duplicates(subset=[FEATURE_COLUMN_NAME, LABEL_COLUMN_NAME], keep=\"first\")\n",
    "\n",
    "        trainer = ModelTrainer(model_params)\n",
    "        df_encoded = trainer.encode_labels(df)\n",
    "        df_train, df_val = trainer.split_to_train_val(df_encoded)\n",
    "\n",
    "        x_train = trainer.tokenize_batch(list(df_train[FEATURE_COLUMN_NAME]))\n",
    "        x_val = trainer.tokenize_batch(list(df_val[FEATURE_COLUMN_NAME]))\n",
    "        y_train = list(df_train[ENCODED_LABEL])\n",
    "        y_val = list(df_val[ENCODED_LABEL])\n",
    "\n",
    "        train_dataset, val_dataset = transform_data_for_training(x_train, y_train, x_val, y_val, model_params)\n",
    "\n",
    "        n_steps = x_train.shape[0] // model_params[TRAIN][BATCH_SIZE]\n",
    "        callbacks = get_callbacks(model_params[TRAIN][CALLBACKS])\n",
    "\n",
    "        # Build and train the model\n",
    "        model = trainer.build_and_train_model(train_dataset, val_dataset, callbacks, n_steps)\n",
    "\n",
    "        evaluator = ModelEvaluator(model, trainer.label_encoder)\n",
    "        y_pred, y_true, metrics = evaluator.calculate_scores(x_val, y_val)\n",
    "        df_val_pred = evaluator.val_data(df_val, y_pred)\n",
    "\n",
    "        artifact_manager = ArtifactManager(artifacts_dir)\n",
    "        artifact_manager.save_artifacts(model, trainer.label_encoder, metrics, df_val_pred)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in training pipeline: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56db9b1-3811-46f0-b077-5256d69dc565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_main(\"data/train\",\"dev.yml\",\"model_artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294145a-5c17-4f90-aa4d-d235ee8ecd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r model_artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf19c8e-8e23-42f0-a986-d8b0b97c81ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_params(model_params_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    The purpose of this function is to load the model parameters from the provided path.\n",
    "    :param model_params_path: GCS path for the model parameters\n",
    "    :return: A dictionary containing the model parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f'Read model params file from: \"{model_params_path}\".')\n",
    "        model_params = None\n",
    "        with file_io.FileIO(model_params_path, \"r\") as f:\n",
    "            model_params = yaml.safe_load(f)\n",
    "        if not model_params:\n",
    "            raise Exception(\n",
    "                f\"Failed to load the model parameters file in the\"\n",
    "                f'following path: \"{model_params_path}\", the file was not found.'\n",
    "            )\n",
    "        logger.info(f\"The model params:\\n{model_params}\")\n",
    "        logger.info(\"Model parameters loaded successfully.\")\n",
    "        return model_params\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model parameters: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578f8f5-1954-40b5-a3a2-3adaeea22649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model_params: Dict[str, Any]):\n",
    "        self.model_params = model_params\n",
    "        self.tokenizer = self.get_tokenizer()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def get_tokenizer(self) -> Any:\n",
    "        try:\n",
    "            logger.info(\"Download the tokenizer from Huggingface.\")\n",
    "            return DistilBertTokenizer.from_pretrained(self.model_params[TRANSFORMERS][TOKENIZER])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            logger.info(\"Perform label encoding.\")\n",
    "            df[ENCODED_LABEL] = self.label_encoder.fit_transform(list(df[LABEL_COLUMN_NAME].values))\n",
    "            logger.info(\n",
    "                f\"The total number of samples: {len(df[ENCODED_LABEL])}; classes: {len(self.label_encoder.classes_)}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in label encoding: {e}\")\n",
    "            raise\n",
    "    def split_to_train_val(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        try:\n",
    "            logger.info(\"Perform train/validation split.\")\n",
    "            return train_test_split(\n",
    "                df,\n",
    "                test_size=self.model_params[DATA][VAL_SIZE],\n",
    "                random_state=self.model_params[RANDOM_STATE],\n",
    "                stratify=list(df[LABEL_COLUMN_NAME]),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during train/validation split: {e}\")\n",
    "            raise\n",
    "\n",
    "    def tokenize_batch(self, texts: List[str], batch_size: int = 32) -> Any:\n",
    "        try:\n",
    "            logger.info(\"Tokenizing batch of texts.\")\n",
    "            tokenized_data = []\n",
    "            for i in range(0, len(texts), self.model_params[ENCODING][BATCH_SIZE]):\n",
    "                batch_text = texts[i:i + self.model_params[ENCODING][BATCH_SIZE]]\n",
    "                tokenized_batch = self.tokenizer(batch_text, max_length=self.model_params[ENCODING][MAX_LENGTH],\n",
    "                                                 padding='max_length', truncation=True, return_tensors='tf')['input_ids']\n",
    "                tokenized_data.append(tokenized_batch)\n",
    "            return tf.concat(tokenized_data, axis=0)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during tokenization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def build_and_train_model(self, train_dataset, val_dataset, callbacks: List[Any], n_steps):\n",
    "        \"\"\"\n",
    "        Build and train the model using TFDistilBertModel and TensorFlow's Keras API.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Building the model.\")\n",
    "            bert_model = TFDistilBertModel.from_pretrained(self.model_params[TRANSFORMERS][MODEL])\n",
    "\n",
    "            input_ids = tf.keras.layers.Input(shape=(self.model_params[ENCODING][MAX_LENGTH],), dtype='int32')\n",
    "            bert_output = bert_model(input_ids)[0]\n",
    "            cls_token = bert_output[:, 0, :]\n",
    "            output = tf.keras.layers.Dense(len(self.label_encoder.classes_), activation='softmax')(cls_token)\n",
    "\n",
    "            model = tf.keras.models.Model(inputs=input_ids, outputs=output)\n",
    "\n",
    "            logger.info(\"Compiling the model.\")\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.model_params[TRAIN][OPTIMIZATION][INIT_LR]),\n",
    "                          loss=SparseCategoricalCrossentropy(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            logger.info(\"Training the model.\")\n",
    "            train_start = time.time()\n",
    "            model.fit(train_dataset,\n",
    "                      validation_data=val_dataset,\n",
    "                      steps_per_epoch=n_steps,\n",
    "                      epochs=self.model_params[TRAIN][EPOCHS],\n",
    "                      batch_size=self.model_params[TRAIN][BATCH_SIZE],\n",
    "                      callbacks=callbacks)\n",
    "            train_time_minutes = round((time.time() - train_start) / 60, 2)\n",
    "            logger.info(f\"The training has finished, took {train_time_minutes} minutes.\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during model building or training: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d06ec46-ef8a-4728-b9a6-ed9547aeff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path: str) -> Any:\n",
    "    \"\"\"Loads the model from the given path and returns it.\"\"\"\n",
    "    logger.info(f'Loading the Model... The path: \"{model_path}\"')\n",
    "    try:\n",
    "        model = tf.saved_model.load(model_path)\n",
    "        model = build_model_from_pb(model)\n",
    "        if not model:\n",
    "            raise Exception(\"Failed to load the model\")\n",
    "        logger.info(\"Model was loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model from {model_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def build_model_from_pb(pb_model: Any) -> Any:\n",
    "    \"\"\"Builds and returns a Keras model from a TensorFlow Hub protobuf model.\"\"\"\n",
    "    try:\n",
    "        input_ids_layer = tf.keras.layers.Input(\n",
    "            shape=(32,),\n",
    "            name=\"input_word\",\n",
    "            dtype=\"int32\",\n",
    "        )\n",
    "        keras_layer = hub.KerasLayer(pb_model, trainable=False)(input_ids_layer)\n",
    "        model = tf.keras.Model([input_ids_layer], keras_layer)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building model from protobuf: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ebd0299-0679-4d68-8065-0993618b524e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelQuantizer:\n",
    "    \"\"\"Class to quantize and save a Keras model.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def quantize_and_save_model(model_path: str, output_dir: str):\n",
    "        \"\"\"Quantizes the Keras model and saves it as a TensorFlow Lite model.\"\"\"\n",
    "        model = load_model(model_path)\n",
    "        print(model.input_shape[0])\n",
    "        #model = tf.keras.models.load_model(model_path)\n",
    "        if model.input_shape[0] is not None:\n",
    "            raise ValueError(\"Model input shape should have a dynamic batch size (None).\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "        converter.allow_custom_ops = True\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        with open(os.path.join(output_dir, 'quantized_model.tflite'), 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        print(f\"Quantized model saved to: {os.path.join(output_dir, 'quantized_model.tflite')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e102d821-e407-4cc4-b58e-29b3d75dd792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = ModelQuantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44710fa-5866-4b4c-ad6d-a36a483e6f15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 165). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1oi8c5p8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1oi8c5p8/assets\n",
      "2024-09-23 10:57:28.736554: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2024-09-23 10:57:28.736672: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2024-09-23 10:57:28.736879: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp1oi8c5p8\n",
      "2024-09-23 10:57:28.791427: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2024-09-23 10:57:28.791486: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp1oi8c5p8\n",
      "2024-09-23 10:57:28.978939: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2024-09-23 10:57:29.953645: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp1oi8c5p8\n",
      "2024-09-23 10:57:30.237304: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 1500423 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: model_quantize/quantized_model.tflite\n"
     ]
    }
   ],
   "source": [
    "t.quantize_and_save_model(\"model_artifacts\",\"model_quantize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f3f441-335e-4446-bbd7-f139bc2bdff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelLoader\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads models and model parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_params\u001b[39m(model_params_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m, in \u001b[0;36mModelLoader\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading model parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel:\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads and builds a TensorFlow model from the given path.\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"Loads models and model parameters.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model_params(model_params_path: str) -> dict:\n",
    "        \"\"\"Loads the model parameters from the given path.\"\"\"\n",
    "        try:\n",
    "            with file_io.FileIO(model_params_path, \"r\") as f:\n",
    "                model_params = yaml.safe_load(f)\n",
    "            if not model_params:\n",
    "                raise FileNotFoundError(f\"Model parameters file not found: {model_params_path}\")\n",
    "            return model_params\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading model parameters: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model_path: str) -> tf.keras.Model:\n",
    "        \"\"\"Loads and builds a TensorFlow model from the given path.\"\"\"\n",
    "        try:\n",
    "            model = tf.saved_model.load(model_path)\n",
    "            return ModelLoader.build_model_from_pb(model)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading model from {model_path}: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def build_model_from_pb(pb_model: tf.Module) -> tf.keras.Model:\n",
    "        \"\"\"Builds a Keras model from a TensorFlow Hub protobuf model.\"\"\"\n",
    "        try:\n",
    "            input_ids_layer = tf.keras.layers.Input(shape=(32,), name=\"input_word\", dtype=\"int32\")\n",
    "            keras_layer = hub.KerasLayer(pb_model, trainable=False)(input_ids_layer)\n",
    "            model = tf.keras.Model([input_ids_layer], keras_layer)\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error building model from protobuf: {e}\")\n",
    "\n",
    "\n",
    "class ModelQuantizer:\n",
    "    \"\"\"Class to quantize and save a Keras model.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def quantize_and_save_model(model_path: str, output_dir: str):\n",
    "        \"\"\"Quantizes the Keras model and saves it as a TensorFlow Lite model.\"\"\"\n",
    "        model = ModelLoader.load_model(model_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        tflite_path = os.path.join(output_dir, 'quantized_model.tflite')\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        print(f\"Quantized model saved to: {tflite_path}\")\n",
    "\n",
    "\n",
    "class InferenceQuantizeModel:\n",
    "    \"\"\"Handles loading the quantized model and making predictions.\"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, model_params_path: str):\n",
    "        self.model_params = ModelLoader.load_model_params(model_params_path)\n",
    "        self.model = self.load_model(f\"{model_path}/quantized_model.tflite\")\n",
    "        self.label_encoder = self.load_label_encoder(f\"{model_path}/{LABEL_ENCODER_PKL}\")\n",
    "        self.tokenizer = self.get_tokenizer()\n",
    "        print(self.input_details)\n",
    "    def load_model(self, model_path: str) -> tf.lite.Interpreter:\n",
    "        \"\"\"Loads the quantized TensorFlow Lite model.\"\"\"\n",
    "        model = tf.lite.Interpreter(model_path=model_path)\n",
    "        model.allocate_tensors()\n",
    "        self.input_details = model.get_input_details()\n",
    "        self.output_details = model.get_output_details()\n",
    "        return model\n",
    "    def get_tokenizer(self) -> Any:\n",
    "        try:\n",
    "            logger.info(\"Download the tokenizer from Huggingface.\")\n",
    "            return DistilBertTokenizer.from_pretrained(\n",
    "                self.model_params[TRANSFORMERS][TOKENIZER]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_label_encoder(self, label_encoder_path: str) -> LabelEncoder:\n",
    "        \"\"\"Loads the label encoder from a file.\"\"\"\n",
    "        try:\n",
    "            with file_io.FileIO(label_encoder_path, mode=\"rb\") as encoder_file:\n",
    "                label_encoder = joblib.load(encoder_file)\n",
    "            if not label_encoder:\n",
    "                raise FileNotFoundError(\"Label encoder not found.\")\n",
    "            return label_encoder\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading label encoder: {e}\")\n",
    "\n",
    "    \n",
    "    def predict(self, inputs: tf.Tensor):\n",
    "        \"\"\" Makes predictions on the provided input and returns the categories and scores.\"\"\"\n",
    "\n",
    "        predictions = []\n",
    "        prob_scores = []\n",
    "\n",
    "        # Loop through each example (assuming the model expects batch size of 1)\n",
    "        for i in range(inputs.shape[0]):\n",
    "            input_data = tf.cast(inputs[i:i+1], dtype=tf.int32)  # Select one example at a time\n",
    "\n",
    "            # Set the input tensor for the TFLite model\n",
    "            self.model.set_tensor(self.input_details[0]['index'], input_data.numpy())  # Convert to NumPy for TFLite\n",
    "\n",
    "            # Run inference\n",
    "            self.model.invoke()\n",
    "\n",
    "            # Get the output tensor from the model\n",
    "            prediction = self.model.get_tensor(self.output_details[0]['index'])\n",
    "\n",
    "            # Collect the prediction and probability scores\n",
    "            predictions.append(prediction)\n",
    "            prob_scores.append(np.max(prediction))\n",
    "\n",
    "        # Convert predictions to appropriate format\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        prediction_transform = np.argmax(predictions, axis=1)\n",
    "        transform_cat = self.label_encoder.inverse_transform(prediction_transform)\n",
    "\n",
    "        return transform_cat, np.array(prob_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CSVProcessor:\n",
    "    \"\"\"Processes CSV files for inference.\"\"\"\n",
    "\n",
    "    def __init__(self, inference_model: InferenceQuantizeModel):\n",
    "        self.inference_model = inference_model\n",
    "\n",
    "    def process_csv(self, file_path: str, prediction_dir: str):\n",
    "        \"\"\"Processes the CSV file and saves predictions.\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        #df = df.head(5)\n",
    "        x_train = self.tokenize_batch(list(df[FEATURE_COLUMN_NAME]))\n",
    "        predictions, prob_scores = self.inference_model.predict(x_train)\n",
    "\n",
    "        df[PREDICTED_CATEGORY] = predictions\n",
    "        df[PROB_SCORES] = prob_scores\n",
    "\n",
    "        output_path = os.path.join(prediction_dir, FILE_NAME_PREFIX + \".csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Predictions saved to: {output_path}\")\n",
    "\n",
    "    def tokenize_batch(self, texts: List[str]) -> Any:\n",
    "        try:\n",
    "            logger.info(\"Tokenizing batch of texts.\")\n",
    "            tokenized_data = []\n",
    "            for i in range(0, len(texts), self.inference_model.model_params[ENCODING][BATCH_SIZE]):\n",
    "                batch_text = texts[i : i + self.inference_model.model_params[ENCODING][BATCH_SIZE]]\n",
    "                tokenized_batch = self.inference_model.tokenizer(\n",
    "                    batch_text,\n",
    "                    max_length=self.inference_model.model_params[ENCODING][MAX_LENGTH],\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"tf\",\n",
    "                )[\"input_ids\"]\n",
    "                tokenized_data.append(tokenized_batch)\n",
    "            return tf.concat(tokenized_data, axis=0)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during tokenization: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class InferenceCLI:\n",
    "    \"\"\"Command-line interface for running inference.\"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, model_params_path: str):\n",
    "        self.inference_model = InferenceQuantizeModel(model_path, model_params_path)\n",
    "\n",
    "    def run_csv_inference(self, csv_path: str, prediction_dir: str):\n",
    "        \"\"\"Runs inference on the specified CSV file.\"\"\"\n",
    "        processor = CSVProcessor(self.inference_model)\n",
    "        processor.process_csv(csv_path, prediction_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee19d1f4-ef3b-4b9d-bbbb-d9562db20170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InferenceCLI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mInferenceCLI\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_quantize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'InferenceCLI' is not defined"
     ]
    }
   ],
   "source": [
    "A = InferenceCLI(\"model_quantize\",\"dev.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e2aea-dabd-4558-9cf3-968c75ac4e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A.run_csv_inference(\"data/test/search_data.csv\",\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba1628-66e2-4ef4-8165-e495401f2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI command function to run inference and quantization\n",
    "def main():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Model Quantization and Inference CLI\")\n",
    "    parser.add_argument(\"--quantize\", action=\"store_true\", help=\"Quantize the model.\")\n",
    "    parser.add_argument(\"--model_path\", type=str, required=True, help=\"Path to the model.\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, help=\"Output directory for the quantized model.\")\n",
    "    parser.add_argument(\"--csv_inference\", type=str, help=\"Path to the CSV file for inference.\")\n",
    "    parser.add_argument(\"--model_params_path\", type=str, required=True, help=\"Path to model params.\")\n",
    "    parser.add_argument(\"--prediction_dir\", type=str, help=\"Directory to save predictions.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.quantize:\n",
    "        ModelQuantizer.quantize_and_save_model(args.model_path, args.output_dir)\n",
    "\n",
    "    if args.csv_inference:\n",
    "        cli = InferenceCLI(args.model_path, args.model_params_path)\n",
    "        cli.run_csv_inference(args.csv_inference, args.prediction_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.11",
   "language": "python",
   "name": "tf2-2-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
